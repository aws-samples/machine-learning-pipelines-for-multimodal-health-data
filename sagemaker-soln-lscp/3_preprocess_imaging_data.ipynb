{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a39ae4",
   "metadata": {},
   "source": [
    "# Preprocess Imaging Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c362878c",
   "metadata": {},
   "source": [
    "For imaging data in this solution, we use the Computed Tomography (CT) series and the corresponding tumor segmentations in the NSCLC Radiogenomic imaging dataset to create patient-level 3-dimensional radiomic features that explain the size, shape and visual attributes of the tumors observed in the study subject’s lungs.\n",
    "\n",
    "Medical imaging data is commonly stored in the [DICOM](https://www.dicomstandard.org/) file format, a standard that combines metadata and pixel data in a single object. For a volumetric scan, such as a lung CT scan, each cross-section slice is typically stored as an individual DICOM file. However, for ML purposes, analyzing 3-dimensional data provides a more wholistic view of the region of interest (ROI), thus providing better predictive values. We convert the scans in DICOM format into [NIfTI](https://nifti.nimh.nih.gov/) format. Thus, we download the DICOM files, store them to S3, then use using [Amazon SageMaker Processing](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) to perform the transformation. Specifically, for each subject and study, we launch a SageMaker Processing job with a custom container to read the 2D DICOM slice files for both the CT scan and tumor segmentation, combine them to 3D volumes, save the volumes in NIfTI format, and write the NIfTI object back to S3. \n",
    "\n",
    "With the medical imaging data in volumetric format, we compute radiomic features describing the tumor region in the same SageMaker Processing job. Finally, the features engineered from each data modality are written to [Amazon SageMaker Feature Store](https://aws.amazon.com/sagemaker/feature-store/), a purpose-built repository for storing ML features. \n",
    "\n",
    "(optional) We run the medical imaging pipeline for each patient in a standalone SageMaker Processing job. This means we are distributing DICOM processing and radiomic feature computation on many SageMaker instances to speed up the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa96b0",
   "metadata": {},
   "source": [
    "## Step 1: Read in the SageMaker JumpStart Solution configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "SOLUTION_CONFIG = json.load(open(\"stack_outputs.json\"))\n",
    "SOLUTION_BUCKET = SOLUTION_CONFIG[\"SolutionS3Bucket\"]\n",
    "REGION = SOLUTION_CONFIG[\"AWSRegion\"]\n",
    "SOLUTION_PREFIX = SOLUTION_CONFIG[\"SolutionPrefix\"]\n",
    "SOLUTION_NAME = SOLUTION_CONFIG[\"SolutionName\"]\n",
    "BUCKET = SOLUTION_CONFIG[\"S3Bucket\"]\n",
    "ECR_REPOSITORY = SOLUTION_CONFIG[\"SageMakerProcessingJobContainerName\"]\n",
    "CONTAINER_BUILD_PROJECT = SOLUTION_CONFIG[\"SageMakerProcessingJobContainerBuild\"]\n",
    "ACCOUNT_ID = SOLUTION_CONFIG[\"AccountID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1ef97",
   "metadata": {},
   "source": [
    "Next step, we install [sagemaker-experiments](https://github.com/aws/sagemaker-experiments) package. We use [SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) to track and manage the SageMaker Processing jobs in this notebook. This allows us to view the status of the jobs for each patient in SageMaker Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b443595",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -I sagemaker-experiments --no-index --find-links file://$PWD/wheelhouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febcbd0",
   "metadata": {},
   "source": [
    "## Step 2: Understand the medical imaging pipeline\n",
    "Working with large amount of DICOM files for a imaging study can be challenging. In this dataset, the source dataset is stored in DICOM format organized by patient/study/imaging sequence, as shown in the directory structure below.\n",
    "\n",
    "```\n",
    "R01-093/  # patient case ID\n",
    "   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.240670240758603778272625719701/  # imaging study ID 1\n",
    "   │   ├── 06-22-1994/  # date of imaging study 1\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.142853031813998654275885617348.json  # metadata for an imaging sequence (eg. CT or PET/CT)\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.142853031813998654275885617348/  # corresponding imaging sequence\n",
    "   │   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.268122123877676186625117813999.dcm  # DICOM file(s) for the imaging sequence\n",
    "   │   │   │   └── ... if there are more *.dcm files\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.143700250022821944637075337096.json\n",
    "   │   └───┴── 1.3.6.1.4.1.14519.5.2.1.4334.1501.143700250022821944637075337096/\n",
    "   │           └── *.dcm\n",
    "   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.891373270862787721463778581588/  # imaging study ID 2\n",
    "   │   ├── 08-31-1994/  # date of imaging study 2\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.127121510050458520043845333993/\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.127121510050458520043845333993.json\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.133664984480262659047625676012/\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.133664984480262659047625676012.json\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.172375466107449789684787272970/\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.172375466107449789684787272970.json\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.204884611948428816419269906683/\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.204884611948428816419269906683.json\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.257442018730697717273113074489/\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.257442018730697717273113074489.json\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.287138696590093616857690167236/\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.287138696590093616857690167236.json\n",
    "   │   │   ├── 1.3.6.1.4.1.14519.5.2.1.4334.1501.327652266327905984847451930257/\n",
    "   └───┴───┴── 1.3.6.1.4.1.14519.5.2.1.4334.1501.327652266327905984847451930257.json\n",
    "```\n",
    "\n",
    "In this study, we focus on the CT scans that are accompanied by a tumor segmentation, even though the PET and PET/CT scans are also available.\n",
    "For each patient and study, we read through the json metadata files to determine if the series has both a CT scan imaging series and a segmentation object. If so, convert each CT scan imaging series, downloaded as a set of 2D DICOM files, to a single 3D NIfTI file. Then perform the DICOM to NIfTI conversion using a python package called [dcmstack](https://dcmstack.readthedocs.io/en/latest/), reading in all the DICOM files, sorting according to spatial slice location, and stacking the slices to create a 3D volume. The 3D volume is then written out in NIfTI format with the [NiBabel](https://nipy.org/nibabel/) python package. For each tumor segmentation DICOM object, use the [Pydicom](https://pydicom.github.io/) package to read in the 3D array, reorient the volume to match that of the corresponding CT scan, and save the output as a NIfTI file.\n",
    "\n",
    "The medical imaging pipeline code is available in [./codebuild/build_container/dcm2nifti_processing.py](./codebuild/build_container/dcm2nifti_processing.py).\n",
    "\n",
    "Note that the segmentation object for some studies were saved to a DICOM object with empty slices cropped out. This results in a mismatch between the number of slices in the CT scan and the corresponding segmentation object. To address this, match the value in the [ImagePositionPatient](https://dicom.innolitics.com/ciods/ct-image/image-plane/00200032) DICOM attribute to align the tumor segmentation to the corresponding location in the CT scan and pad the segmentation with zeros to have identical number of slices. \n",
    "\n",
    "The figure below shows example views of overlaying the tumor mask in yellow with transparency on the CT scan for a study (case ID R01-093).\n",
    "\n",
    "![ct-R01-093-1](../docs/R01-093_06-22-1994_ortho-view.png)\n",
    "![ct-R01-093-2](../docs/R01-093_06-22-1994_z-view.png)\n",
    "\n",
    "Once the 3D volumes are constructed in NIfTI format, we compute the radiomic features describing the tumors in the annotated CT scans using the [pyradiomics](https://www.radiomics.io/pyradiomics.html) library. Using the library, we extract [120 radiomic features of 8 classes](https://pyradiomics.readthedocs.io/en/latest/features.html) such as statistical representations of the distribution and co-occurrence of the intensity within tumorous region of interest, and shape-based measurements describing the tumor morphologically. The computation of the radiomic features is performed volumetrically by providing the converted NIfTI images to the `RadiomicsFeatureExtractor` class in the `compute_features()` function in the [./codebuild/build_container/radiomics_utils.py](./codebuild/build_container/radiomics_utils.py) script.\n",
    "\n",
    "```python\n",
    "df = utils.compute_features(imageName, maskName)\n",
    "```\n",
    "\n",
    "The radiomic features computed from a pair of CT scan and segmentation are then ingested into a feature group in SageMaker Feature Store in [dcm2nifti_processing.py](./codebuild/build_container/dcm2nifti_processing.py).\n",
    "\n",
    "```python \n",
    "feature_group = utils.create_feature_group(...)\n",
    "feature_group.ingest(data_frame=df, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d811e4",
   "metadata": {},
   "source": [
    "## Step 3: Build a Container Image through Codebuild\n",
    "\n",
    "To run the medical imaging pipeline using SageMaker Processing, we need to build a custom Docker container image from the [Dockerfile](./codebuild/build_container/Dockerfile). We are going to provisioned codebuild project to build an image and hold it in the Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad67f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import run_codebuild\n",
    "\n",
    "run_codebuild.build(CONTAINER_BUILD_PROJECT, boto3.session.Session(region_name=REGION))\n",
    "ecr_image_uri = f\"{ACCOUNT_ID}.dkr.ecr.{REGION}.amazonaws.com/{ECR_REPOSITORY}:latest\"\n",
    "print(ecr_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcbcf5",
   "metadata": {},
   "source": [
    "## Step 4: Set up SageMaker Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e70c5",
   "metadata": {},
   "source": [
    "We set up an [experiment](https://sagemaker-experiments.readthedocs.io/en/latest/experiment.html) using SageMaker Experiments to hold information of the medical imaging processing jobs. We can then easily track the jobs, and status in the **Experiments and trials** resources in the Studio UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865913f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "\n",
    "SM_BOTO = boto3.client('sagemaker', config=Config(connect_timeout=5, read_timeout=60, retries={'max_attempts': 20}), region_name=REGION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import gmtime, strftime\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "experiment_name = f'{SOLUTION_PREFIX}-nsclc'\n",
    "trial_name = f'{SOLUTION_PREFIX}-trial'\n",
    "\n",
    "\n",
    "# create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = Experiment.load(experiment_name=experiment_name,\n",
    "                                 sagemaker_boto_client=SM_BOTO)\n",
    "except Exception as e:\n",
    "    if \"ResourceNotFound\" in str(e):\n",
    "        experiment = Experiment.create(\n",
    "            experiment_name=experiment_name,\n",
    "            sagemaker_boto_client=SM_BOTO,\n",
    "            description='Lung cancer survival prediction using multi-modal Non Small Cell Lung Cancer (NSCLC) Radiogenomic dataset')\n",
    "        print(f'{experiment_name} experiment is created!')\n",
    "\n",
    "\n",
    "# create the trial if it doesn't exist\n",
    "try:\n",
    "    exp_trial = Trial.load(trial_name=trial_name,\n",
    "                           sagemaker_boto_client=SM_BOTO)\n",
    "except Exception as e:\n",
    "    if \"ResourceNotFound\" in str(e):\n",
    "        exp_trial = Trial.create(experiment_name=experiment_name, \n",
    "                                 sagemaker_boto_client=SM_BOTO,\n",
    "                                 trial_name=trial_name)\n",
    "        print(f'{exp_trial} trial is created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e6e928",
   "metadata": {},
   "source": [
    "## Step 5: Create SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7934684b",
   "metadata": {},
   "source": [
    "We implement a function `launch_processing_job()` to launch a SageMaker processing job with the container image `ecr_image_uri` created in the previous step. \n",
    "\n",
    "In this solution, we run only the patients that have proper CT scans and tumor segmentations. There are a few imaging studies that come without segmentations or could not be constructed into NIfTI 3D volumes correctly in the original dataset, we remove those studies from the S3 bucket location to avoid spinning up unnecessary SageMaker Processing jobs. To gracefully skipping those studies, we put `script_processor.run()` in a try/except clause. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14818b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session(sagemaker_client=SM_BOTO)\n",
    "dict_processor = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b14c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "\n",
    "def launch_processing_job(subject, input_data_s3, output_data_s3, feature_group_name, offline_store_s3uri, retries):\n",
    "    \n",
    "    exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "    jobname = f'{SOLUTION_PREFIX}-{subject}-{exp_datetime}' \n",
    "\n",
    "    experiment_config={'ExperimentName': experiment_name,\n",
    "                       'TrialName': trial_name,\n",
    "                       'TrialComponentDisplayName': f'ImageProcessing-{subject}'}\n",
    "\n",
    "    inputs = [ProcessingInput(input_name='DICOM',\n",
    "                              source=f'{input_data_s3}/{subject}', \n",
    "                              destination='/opt/ml/processing/input')]\n",
    "\n",
    "    outputs = [ProcessingOutput(output_name=i,\n",
    "                                source='/opt/ml/processing/output/%s' % i,\n",
    "                                destination=os.path.join(output_data_s3, i)) \n",
    "               for i in ['CT-Nifti', 'CT-SEG', 'PNG']]\n",
    "\n",
    "    arguments = ['--subject', subject, \n",
    "                 '--feature_group_name', feature_group_name, \n",
    "                 '--offline_store_s3uri', offline_store_s3uri]\n",
    "\n",
    "    script_processor = ScriptProcessor(command=['python3'],\n",
    "                                       image_uri=ecr_image_uri,\n",
    "                                       role=sagemaker.get_execution_role(),\n",
    "                                       instance_count=1,\n",
    "                                       instance_type='ml.r5.large',\n",
    "                                       volume_size_in_gb=5,\n",
    "                                       sagemaker_session=sagemaker_session)\n",
    "    current_retry = 1\n",
    "    while True and current_retry <= retries:\n",
    "        try:\n",
    "            script_processor.run(code='codebuild/build_container/dcm2nifti_processing.py',\n",
    "                                 inputs=inputs,\n",
    "                                 outputs=outputs,\n",
    "                                 arguments=arguments,\n",
    "                                 job_name=jobname,\n",
    "                                 experiment_config=experiment_config,\n",
    "                                 wait=False,\n",
    "                                 logs=False)\n",
    "            return script_processor\n",
    "\n",
    "        except ClientError as e:\n",
    "            if \"No S3 objects found under S3 URL\" in str(e):\n",
    "                print(\"Bad input data has been removed from s3, processing is not created!\")\n",
    "                return None\n",
    "            elif \"ResourceLimitExceeded\" in str(e):\n",
    "                if current_retry == retries:\n",
    "                    raise\n",
    "                else:\n",
    "                    print(\"Resource reaches limit, please retry after 30 seconds ...\")\n",
    "                    current_retry += 1\n",
    "                    time.sleep(30)\n",
    "                    continue\n",
    "            else:\n",
    "                print(f'Processing job with {subject} subject is not created successfully! {e}.')\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e4bac",
   "metadata": {},
   "source": [
    "## Step 6: Run Processing Jobs for All Patients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc405c4",
   "metadata": {},
   "source": [
    "To launch the SageMaker Processing jobs for multiple patients in parallel efficiently, we will run `launch_processing_job()` in a for loop, with each call submits a processing job to run on one `ml.r5.large` instance. In this case, we need to work with the service quota cleverly. \n",
    "\n",
    "The default service quota for *Number of instances across processing jobs* and *number of ml.r5.large instances* are 4 as documented in [Service Quota page](https://docs.aws.amazon.com/general/latest/gr/sagemaker.html#limits_sagemaker). If your account has a higher limit, you may run a higher number of simultaneous processing jobs (therefore faster completion time). To request a quota increase, please follow the [guidance](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html). \n",
    "\n",
    "We implemented a function `wait_for_instance_quota()` to check for the current job count that is in `InProgress` state and limit the total count in this experiment to the value set in `job_limit`. If the total running job count is at the limit, the function waits number of seconds specified in `wait` argument and check the job count again. This is to account for account level SageMaker quota that may cause error in the for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e8cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_jobs(dict_processor):\n",
    "    for key in list(dict_processor):\n",
    "        if dict_processor[key]:\n",
    "            status = dict_processor[key].jobs[-1].describe()['ProcessingJobStatus']\n",
    "            if status in [\"Completed\", \"Failed\", \"Stopped\"]:\n",
    "                del dict_processor[key]\n",
    "        else:\n",
    "            del dict_processor[key] # when no ProcessingJob created, i.e., None\n",
    "    return len(dict_processor)\n",
    "\n",
    "\n",
    "def wait_for_instance_quota(dict_processor, job_limit=4, wait=30):   \n",
    "    job_count = query_jobs(dict_processor)    \n",
    "    while job_count >= job_limit:\n",
    "        print(f'Current total running jobs {job_count} is reaching the limit {job_limit}. Waiting {wait} seconds...')\n",
    "        time.sleep(wait)\n",
    "        job_count = query_jobs(dict_processor)\n",
    "        \n",
    "    print(f'Current total running jobs {job_count} is below {job_limit}. Proceeding...')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f904b47",
   "metadata": {},
   "source": [
    "Here, we set `job_limit` equals to 20 as an example, and it takes around 1 hour to complete all the processing jobs. You would expect a longer running time if you decrease the `job_limit`. \n",
    "\n",
    "We will be running this workflow for all the `RO1` subjects as defined in `subject_list` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where the DICOM files\n",
    "input_data_bucket = f\"{SOLUTION_BUCKET}-{REGION}\" \n",
    "input_data_prefix = f\"{SOLUTION_NAME}/data/nsclc_radiogenomics\"\n",
    "\n",
    "output_data_bucket=BUCKET\n",
    "output_data_prefix=\"nsclc_radiogenomics\"\n",
    "\n",
    "input_dicom_dir = f\"s3://{input_data_bucket}/{input_data_prefix}\"\n",
    "output_nifti_dir = f\"s3://{output_data_bucket}/{output_data_prefix}\"\n",
    "\n",
    "imaging_feature_group_name = f'{SOLUTION_PREFIX}-imaging-feature-group'\n",
    "%store imaging_feature_group_name\n",
    "\n",
    "offline_store_s3uri = '%s/multimodal-imaging-featurestore' % output_nifti_dir\n",
    "\n",
    "subject_list = ['R01-%03d'%i for i in range(1, 164)]\n",
    "\n",
    "for subject in subject_list:\n",
    "    print(subject)\n",
    "    wait_for_instance_quota(dict_processor, job_limit=20, wait=30)\n",
    "    dict_processor[subject] = launch_processing_job(subject, input_dicom_dir, output_nifti_dir, imaging_feature_group_name, offline_store_s3uri, 10)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11786a2",
   "metadata": {},
   "source": [
    "Earlier we created an experiment to track our Processing Jobs. Let's take a look at how we can navigate to Processing Jobs from Experiments inside of SageMaker Studio.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <center>\n",
    "                <img src=\"../docs/exp_1_icon.png\" alt=\"SageMaker Resources Icon\" style=\"width: 200px;\"/>\n",
    "                <p style=\"margin: 20px 80px 0px\">To locate Experiments, first click on the SageMaker resources icon on the left side of the screen. The icon should be the last one on the left.</p>\n",
    "            </center>\n",
    "        </td>\n",
    "        <td>\n",
    "            <center>\n",
    "                <img src=\"../docs/exp_2_drop_down.png\" alt=\"Resource Drop Down\" style=\"width: 200px;\"/>\n",
    "                <p style=\"margin: 20px 80px 0px\">Inside of SageMaker resources, there is a drop-down underneath the SageMaker resources title and subject. Click the drop down. Here is where you can navigate to differernt SageMaker resources that are integrated with SageMaker Studio. Since we are looking for Experiments, select <b>Experiments and trials</b> from the drop down.</p>\n",
    "            </center>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <center>\n",
    "                <img src=\"../docs/exp_3_select_experiment.png\"       alt=\"Select an Experiment\"           style=\"width: 200px;\"/>\n",
    "                <p style=\"margin: 20px 80px 0px\">Inside of the Experiments and trials, we can see Experiments that were created. Let's navigate and double click on the first experiment titled <b>sagemaker-soln-lcsp-js-some random characters-nsclc</b> located under Name.</p>\n",
    "            </center>\n",
    "        </td>\n",
    "        <td>\n",
    "            <center>\n",
    "                <img src=\"../docs/exp_4_select_trial.png\"            alt=\"Select a Trial\"                 style=\"width: 200px;\"/>\n",
    "                <p style=\"margin: 20px 80px 0px\">Here is where we can find different trials within our Experiment. A trial consists of set of steps called trial components, and an example of a trial component is a Processing Job. Under Name, double click on the trial titled <b>sagemaker-soln-lcsp-js-some random characters-trial</b>.</p>\n",
    "            </center>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <center>\n",
    "                <img src=\"../docs/exp_5_select_trial_components.png\" alt=\"Select Trial Component\"         style=\"width: 200px;\"/>\n",
    "                <p style=\"margin: 20px 80px 0px\">This will take you to the Trial Components Section. Here is where we can find all of the Processing Jobs that were run for this trial. Double click on the first Processing Job titled <b>ImageProcessing-R01-some number</b> under Name.</p>\n",
    "            </center>\n",
    "        </td>\n",
    "        <td>\n",
    "            <center>\n",
    "                <img src=\"../docs/exp_6_processing_job_metrics.png\"         alt=\"Understanding a Processing Job\" style=\"width: 400px;\"/>\n",
    "                <p style=\"margin: 20px 80px 0px\">Within this Trial Component, you can select a number of items that were tracked for this Processing Job like: parameters, artifacts, and settings. </p>\n",
    "            </center>    \n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de46d2f",
   "metadata": {},
   "source": [
    "Programmatically, we can search for experiments, trials, and trial components using the SearchExpression method from the smexperiments library. Below shows us how to search and filter Processing jobs that are `InProgress` and the cell after shows an example of finding Processing jobs that may have `Failed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.search_expression import Filter, Operator, SearchExpression\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "\n",
    "search_expression_processing = SearchExpression(filters=[Filter(name='Status.PrimaryStatus',\n",
    "                                                                operator=Operator.EQUALS,\n",
    "                                                                value='InProgress'),\n",
    "                                                         Filter(name='Parents.ExperimentName', \n",
    "                                                                operator=Operator.EQUALS, \n",
    "                                                                value=experiment_name),\n",
    "                                                         Filter(name='Source.SourceType', \n",
    "                                                                operator=Operator.EQUALS, \n",
    "                                                                value='SageMakerProcessingJob')])\n",
    "check = True\n",
    "\n",
    "while check:\n",
    "    num_processing_jobs = len(list(TrialComponent.search(search_expression=search_expression_processing)))\n",
    "    if num_processing_jobs == 0:\n",
    "        check = False\n",
    "    else:\n",
    "        print(f\"{num_processing_jobs} processing jobs are still running. Check again after 30 sec ...\") # add number of jobs\n",
    "        time.sleep(30)\n",
    "print(\"All processing jobs finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98016d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_expression_failed = SearchExpression(filters=[Filter(name='Status.PrimaryStatus',\n",
    "                                                                operator=Operator.EQUALS,\n",
    "                                                                value='Failed'),\n",
    "                                                         Filter(name='Parents.ExperimentName', \n",
    "                                                                operator=Operator.EQUALS, \n",
    "                                                                value=experiment_name),\n",
    "                                                         Filter(name='Source.SourceType', \n",
    "                                                                operator=Operator.EQUALS, \n",
    "                                                                value='SageMakerProcessingJob')])\n",
    "\n",
    "\n",
    "num_failed_jobs = len(list(TrialComponent.search(search_expression=search_expression_failed)))\n",
    "print(f\"{num_failed_jobs} jobs failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215311a",
   "metadata": {},
   "source": [
    "Retrieve the experiment data for analysis and review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c73554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import analytics\n",
    "\n",
    "trial_component_analytics = analytics.ExperimentAnalytics(experiment_name=experiment_name)\n",
    "\n",
    "analytic_table = trial_component_analytics.dataframe()\n",
    "analytic_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb43b4",
   "metadata": {},
   "source": [
    "## Next Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00dbe15",
   "metadata": {},
   "source": [
    "Next, we'll move on to start the ML modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56ef02c",
   "metadata": {},
   "source": [
    "Click here to [continue](./4_train_test_model.ipynb)."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
