{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Deployment\n",
    "In this notebook, we demonstrate how to access multimodal features, including genomics features, clinical features, and radiomics features, from SageMaker Feature Store. We train a XGBoost model to predict the survival status of patients diagnosed with non-small cell lung cancer. In the end we host the trained model for inference on the testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read in the SageMaker JumpStart Solution configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "SOLUTION_CONFIG = json.load(open(\"stack_outputs.json\"))\n",
    "SOLUTION_BUCKET = SOLUTION_CONFIG[\"SolutionS3Bucket\"]\n",
    "REGION = SOLUTION_CONFIG[\"AWSRegion\"]\n",
    "SOLUTION_PREFIX = SOLUTION_CONFIG[\"SolutionPrefix\"]\n",
    "SOLUTION_NAME = SOLUTION_CONFIG[\"SolutionName\"]\n",
    "BUCKET = SOLUTION_CONFIG[\"S3Bucket\"]\n",
    "ECR_REPOSITORY = SOLUTION_CONFIG[\"SageMakerProcessingJobContainerName\"]\n",
    "CONTAINER_BUILD_PROJECT = SOLUTION_CONFIG[\"SageMakerProcessingJobContainerBuild\"]\n",
    "ACCOUNT_ID = SOLUTION_CONFIG[\"AccountID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setting up access to the multi-modal feature store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will get the feature groups created by the processing notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "\n",
    "boto_session = boto3.Session(region_name=REGION)\n",
    "sagemaker_client = boto_session.client(service_name=\"sagemaker\", region_name=REGION)\n",
    "featurestore_runtime = boto_session.client(service_name=\"sagemaker-featurestore-runtime\", region_name=REGION)\n",
    "\n",
    "feature_store_session = Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "%store -r genomic_feature_group_name\n",
    "%store -r clinical_feature_group_name\n",
    "%store -r imaging_feature_group_name\n",
    "\n",
    "genomic_feature_group = FeatureGroup(name=genomic_feature_group_name, sagemaker_session=feature_store_session)\n",
    "clinical_feature_group = FeatureGroup(name=clinical_feature_group_name, sagemaker_session=feature_store_session)\n",
    "imaging_feature_group = FeatureGroup(name=imaging_feature_group_name, sagemaker_session=feature_store_session)\n",
    "\n",
    "prefix = 'multi-model-health-ml'\n",
    "fs_output_location = f's3://{BUCKET}/{prefix}/feature-store-queries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_query = genomic_feature_group.athena_query()\n",
    "clinical_query = clinical_feature_group.athena_query()\n",
    "imaging_query = imaging_feature_group.athena_query()\n",
    "\n",
    "genomic_table = genomic_query.table_name\n",
    "clinical_table = clinical_query.table_name\n",
    "imaging_table = imaging_query.table_name\n",
    "\n",
    "print('Table names:')\n",
    "print(genomic_table)\n",
    "print(clinical_table)\n",
    "print(imaging_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because medical imaging processings take some time to complete for all patients and because it takes 5-10 minutes for the the feature store to synchronize the offline store, imaging features may not be available in the offline store for us to create a complete trainnig dataset. To make sure the medical imaging features are available for training, let's check the total entry count and wait before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "query_string = f'SELECT COUNT(subject) FROM \"{imaging_table}\"'\n",
    "imaging_data_count = 0\n",
    "total_imaging_entries = 123\n",
    "total_waiting_time = 600\n",
    "current_waiting_time = 0\n",
    "\n",
    "while imaging_data_count < total_imaging_entries and current_waiting_time < total_waiting_time:\n",
    "    imaging_query.run(query_string=query_string, output_location=fs_output_location)\n",
    "    imaging_query.wait()\n",
    "    imaging_data_count = imaging_query.as_dataframe().values[0][0]\n",
    "    print(f'Total feature count in imaging offline store: {imaging_data_count}')\n",
    "    if imaging_data_count < total_imaging_entries:\n",
    "        print(f'We expect {total_imaging_entries} entries in the offline store. Wait 15 seconds.')\n",
    "        time.sleep(15)\n",
    "        current_waiting_time += 15\n",
    "    else:\n",
    "        print('We have all the imaging features. Please proceed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Query against multi-modal feature store to create training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, we will create a ML dataset by querying the multimodal feature groups and joining them. You can choose to query features from 3 pre-determined combinations with the `data_type` argument in the `get_features()` function: \n",
    "1. genomic features only\n",
    "2. genomic + clinical features\n",
    "3. genomic + clinical + imaging features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_data_type = ('genomic', 'genomic-clinical', 'genomic-clinical-imaging')\n",
    "\n",
    "def get_features(data_type, output_location):  \n",
    "    if (data_type == 'genomic'):\n",
    "        query_string = f'SELECT * FROM \"{genomic_table}\"'\n",
    "        print(query_string)\n",
    "        genomic_query.run(query_string=query_string, output_location=output_location)\n",
    "        genomic_query.wait()\n",
    "        dataset = genomic_query.as_dataframe()\n",
    " \n",
    "        # Drop features\n",
    "        drop_features = ['case_id', 'eventtime', 'write_time', 'api_invocation_time', 'is_deleted'] \n",
    "        dataset = dataset.drop(drop_features, axis = 1) \n",
    "        \n",
    "    elif (data_type == 'genomic-clinical'):\n",
    "        query_string = f'''SELECT * FROM \"{clinical_table}\"\n",
    "                           LEFT JOIN \"{genomic_table}\" ON \"{clinical_table}\".case_id = \"{genomic_table}\".case_id'''\n",
    "        print(query_string)\n",
    "\n",
    "        genomic_query.run(query_string=query_string, output_location=output_location)\n",
    "        genomic_query.wait()\n",
    "        dataset = genomic_query.as_dataframe()\n",
    "        \n",
    "        # Drop features\n",
    "        drop_features = ['case_id', 'case_id.1', \n",
    "                  'eventtime', 'write_time', 'api_invocation_time', 'is_deleted',\n",
    "                  'eventtime.1', 'write_time.1', 'api_invocation_time.1', 'is_deleted.1']\n",
    "        dataset = dataset.drop(drop_features, axis = 1)\n",
    "        \n",
    "    elif (data_type == 'genomic-clinical-imaging'):\n",
    "        query_string = f'''SELECT \"{genomic_table}\".*, \"{clinical_table}\".*, \"{imaging_table}\".* \n",
    "                           FROM \"{genomic_table}\"\n",
    "                               LEFT OUTER JOIN \"{clinical_table}\" ON \"{clinical_table}\".case_id = \"{genomic_table}\".case_id\n",
    "                               LEFT OUTER JOIN \"{imaging_table}\" ON \"{clinical_table}\".case_id = \"{imaging_table}\".subject\n",
    "                           ORDER BY \"{clinical_table}\".case_id ASC'''\n",
    "        print(query_string)\n",
    "        \n",
    "        genomic_query.run(query_string=query_string, output_location=output_location)\n",
    "        genomic_query.wait()\n",
    "        dataset = genomic_query.as_dataframe()\n",
    "        \n",
    "        # Drop features\n",
    "        drop_features = ['case_id', 'case_id.1', \n",
    "                  'eventtime', 'write_time', 'api_invocation_time', 'is_deleted',\n",
    "                  'eventtime.1', 'write_time.1', 'api_invocation_time.1', 'is_deleted.1', \n",
    "                  'eventtime.2', 'write_time.2', 'api_invocation_time.2', 'is_deleted.2']\n",
    "        drop_features_img = ['imagename', 'maskname', 'scandate', 'subject']\n",
    "        drop_features_img += [i for i in dataset.columns.tolist() if 'diagnostics' in i]\n",
    "        \n",
    "        dataset = dataset.drop(drop_features + drop_features_img, axis = 1)\n",
    "        \n",
    "        \n",
    "    elif data_type not in supported_data_type:\n",
    "        raise KeyError(f'data_type {data_type} is not supported for this analysis.')\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will generate a dataset by combining the features from gemonic, clinical and imaging tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'genomic-clinical-imaging'\n",
    "\n",
    "dataset = get_features(data_type, fs_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query result from athena will be stored in S3 bucket, so that in the following steps, we can use it from the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to csv in S3 without headers and index column.\n",
    "filename=f'{data_type}-dataset.csv'\n",
    "dataset_uri_prefix = f's3://{BUCKET}/{prefix}/training_input/'\n",
    "\n",
    "dataset.to_csv(filename, header=False, index=False)\n",
    "s3_client = boto3.client('s3', region_name=REGION)\n",
    "s3_client.upload_file(filename, BUCKET, f'{prefix}/training_input/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X for features, y for target\n",
    "X = dataset.drop(['survivalstatus'], axis=1)\n",
    "y = dataset['survivalstatus']\n",
    "\n",
    "# replacing NaNs with zeros\n",
    "X.fillna(value=0., inplace=True)\n",
    "\n",
    "print ('Number of samples in multimodal data: ', dataset.shape[0])\n",
    "print ('Number of features in multimodal data: ', dataset.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preprocessing\n",
    "We randomly shuffle the full dataset and divide it into 80% for training and 20% for testing the model. Later we further split the training data into 80% for training and 20% for validating the model.\n",
    "We perform feature scaling to normalize the range of independent features. This is for performing principal component analysis (PCA) on the features to identify the most discriminative features, even though XGBoost algorithm does not require feature normalization.\n",
    "With PCA, we identify the top principal components that contribute to 99% (**to update**) variance in the data and reduce the feature dimensionality to a smaller number of principal components. Then we furhter analyze the importance of each data domain for the given predictive task by generating feature importance heat maps and correlation circles. We will explain in more detail in corresponding cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features with `StandardScaler` from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Add Feature Scaling\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_trainval)\n",
    "X_trainval_scaled = sc.transform(X_trainval)\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PCA for Dimensioanality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set variance threshold for PCA to 99%\n",
    "pca_threshold = 0.99\n",
    "pca = PCA(n_components=pca_threshold, random_state=0)\n",
    "X_trainval_pca = pca.fit_transform(X_trainval_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(rc={'figure.figsize':(14,10)})\n",
    "\n",
    "F = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(F)\n",
    "plt.xlabel('Number of PCs')\n",
    "plt.ylabel('Explanation of Variance Ratio')\n",
    "print('Number of principal components selected: ', len(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Feature Importance\n",
    "To determine the importance the features belonging to different modalities, we explore the composition of the principal components from the multimodal features. These principal components are linear combinations of the original features taken from each modality. They constitute a set of linearly uncorrelated features that describe the variance in the data. The first principal component explains the most variance, followed by subsequent principal components each explaining successively less of the variance in the data.\n",
    "\n",
    "We plot the feature importance (level of variance in principal components) as a heat map. The heat map demonstrates the variance explained by each principal compnent, color-coded by data modality, for the top 20 principal components. The X-axis denotes principal components, Y-axis denotes features obtained from different domains (genomic, clinical, and medical imaging), and cell intensity denotes that feature’s level of variance (normalized value) for the corresponding principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Get column header for visualization\n",
    "header = dataset.columns.values\n",
    "\n",
    "names = list(header)\n",
    "names = names[1:]\n",
    "\n",
    "# PCA and feature importance\n",
    "# Indices depend on the above join operation on multiple tables  \n",
    "indices = {'imaging': 108, 'clinical': 21}\n",
    "\n",
    "features_projected = abs(pca.components_).T\n",
    "fn = MinMaxScaler().fit_transform(features_projected*pca.explained_variance_ratio_)\n",
    "\n",
    "# Select top 20 PCs to plot\n",
    "n_pc = 20\n",
    "fn = fn[:,:n_pc]\n",
    "\n",
    "fn_genomic = fn[0:indices['clinical'], :]\n",
    "fn_clinical = fn[indices['clinical']:indices['imaging'], :]\n",
    "fn_imaging = fn[indices['imaging']:, :]\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, gridspec_kw={'wspace':0.025, 'hspace':0.052, 'height_ratios': [indices['clinical'], indices['imaging']-indices['clinical']+1, X.shape[1]-indices['imaging']]})\n",
    "\n",
    "sns.heatmap(fn_genomic, ax=ax1, xticklabels=False, yticklabels=False, cmap=\"Blues\", cbar=True)\n",
    "sns.heatmap(fn_clinical, ax=ax2, xticklabels=False, yticklabels=False, cmap=\"Reds\", cbar=True)\n",
    "sns.heatmap(fn_imaging, ax=ax3, yticklabels=False, cmap=\"Greens\", cbar=True)\n",
    "ax3.set_xlabel('Principal Components')\n",
    "ax3.set_ylabel('Medical Imaging')\n",
    "ax2.set_ylabel('Clinical')\n",
    "ax1.set_ylabel('Genomic')\n",
    "plt.subplots_adjust(wspace=None, hspace=None)\n",
    "plt.savefig('pca_matrix.png', dpi=450, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can observe the following in the heat map. For the first few components, medical imaging features are most heavily weighted and have higher variance (higher intensity in heat map) than clinical and genomic features, indicating the importance of medical imaging features for survival outcome prediction. However, from the third principal component on, we see the weighting of the clinical and genomic modality. This shows how the clinical and genomic modalities explain additional variance beyond what is captured in the medical images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Correlation Circle\n",
    "We also plot a variable correlation circle to understand how each original feature correlates to the principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "# Principal components to plot\n",
    "pc_n1 = 4\n",
    "pc_n2 = 5\n",
    "r = np.linalg.norm(fn[:,pc_n1:pc_n2], axis=1)\n",
    "\n",
    "ix_r = np.argsort(-r)\n",
    "# Select top 30 features\n",
    "top_r = 30\n",
    "for i in range(top_r):\n",
    "    plt.plot([0, fn[ix_r[i],0]], [0, fn[ix_r[i],1]], lw=3., label=names[ix_r[i]])\n",
    "\n",
    "temp = np.max(r)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "circ = plt.Circle((0, 0), radius=temp, edgecolor=None, facecolor='w')\n",
    "ax.add_patch(circ)\n",
    "\n",
    "plt.xlim([-temp, temp])\n",
    "plt.ylim([-temp, temp])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('correlationcircle.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable correlation circle depicts correlation between top 25 features or variables of the fourth and fifth principal components. The angle between a pair of vectors indicates the level of correlation between them. A small angle indicates positive correlation, whereas an angle close to 180 degrees indicates negative correlation. The distance between the feature and the origin indicates how well the feature is represented in the plot. For these principal components, the top 25 features are obtained from all three domains: medical imaging (eg. original_glcm_clustershade), genomic (eg. gdf15), and clinical (eg. pack_years, histology_adinocarcinoma)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Split into training and validation data\n",
    "X_train_pca, X_val_pca, y_train, y_val = train_test_split(X_trainval_pca, y_trainval, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Number of training samples: %d'%len(y_train))\n",
    "print('Number of validation samples: %d'%len(y_val))\n",
    "\n",
    "# Create training, validation, and test data by adding label as the first column and removing headers\n",
    "X_train_pca = pd.DataFrame.from_records(X_train_pca)\n",
    "train_data = pd.concat([y_train.reset_index(drop=True), X_train_pca.reset_index(drop=True)], axis=1)\n",
    "\n",
    "X_val_pca = pd.DataFrame.from_records(X_val_pca)\n",
    "validation_data = pd.concat([y_val.reset_index(drop=True), X_val_pca.reset_index(drop=True)], axis=1)\n",
    "\n",
    "X_test_pca = pd.DataFrame.from_records(X_test_pca)\n",
    "test_data = pd.concat([y_test.reset_index(drop=True), X_test_pca.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the model\n",
    "As we are predicting the survival status (death or alive) using the multimodal features, it is a binary classification problem. We use the SageMaker built-in XGBoost algorithm, which is highly optimized for AWS compute infrastructure, and SageMaker managed training feature to train a model to predict the survival status. We save the processed dataframes (`train_data`, `validation_data`, and `test_data`) as CSV files and upload to S3 bucket for model training. When using SageMaker managed training, one compute instance (`ml.m5.xlarge` in this solution) is provisioned to run the model training, instead of using compute resource on the notebook. Training and validation datasets in S3 bucket are automatically downloaded to the training instance. The compute instance will be shutdown once the training completes without any manual termination and management overhead.\n",
    "\n",
    "We record the training job as a trial component to the same experiment and trial we created in [3_preprocessing_imaging_data](./3_preprocess_imaging_data.ipynb).ipynb to preserve the lineage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "experiment_name = f'{SOLUTION_PREFIX}-nsclc'\n",
    "trial_name = f'{SOLUTION_PREFIX}-trial'\n",
    "\n",
    "# create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = Experiment.load(experiment_name=experiment_name)\n",
    "except Exception as e:\n",
    "    if \"ResourceNotFound\" in str(e):\n",
    "        experiment = Experiment.create(\n",
    "            experiment_name=experiment_name,\n",
    "            sagemaker_boto_client=SM_BOTO,\n",
    "            description='Lung cancer survival prediction using multi-modal Non Small Cell Lung Cancer (NSCLC) Radiogenomic dataset')\n",
    "        print(f'{experiment_name} experiment is created!')\n",
    "\n",
    "\n",
    "# create the trial if it doesn't exist\n",
    "try:\n",
    "    exp_trial = Trial.load(trial_name=trial_name)\n",
    "except Exception as e:\n",
    "    if \"ResourceNotFound\" in str(e):\n",
    "        exp_trial = Trial.create(experiment_name=experiment_name, \n",
    "                                 sagemaker_boto_client=SM_BOTO,\n",
    "                                 trial_name=trial_name)\n",
    "        print(f'{exp_trial} trial is created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.session import TrainingInput\n",
    "\n",
    "container = retrieve(\"xgboost\", region=REGION, version='1.2-1')\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    sagemaker.get_execution_role(), \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m5.xlarge',\n",
    "                                    output_path='s3://{}/{}/training-output'.format(BUCKET, prefix),\n",
    "                                    base_job_name='sagemaker-soln-lcsp-js-training',\n",
    "                                    sagemaker_session=sagemaker.Session())\n",
    "\n",
    "xgb.set_hyperparameters(eta=0.1, objective='reg:logistic', num_round=10) \n",
    "\n",
    "# Save data\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)\n",
    "test_data.to_csv('test.csv', header=False, index=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(BUCKET).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(BUCKET).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')\n",
    "boto3.Session().resource('s3').Bucket(BUCKET).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "\n",
    "s3_input_train = TrainingInput(s3_data='s3://{}/{}/train/train.csv'.format(BUCKET, prefix), \n",
    "                               content_type='text/csv')\n",
    "s3_input_validation = TrainingInput(s3_data='s3://{}/{}/validation/validation.csv'.format(BUCKET, prefix), \n",
    "                                    content_type='text/csv')\n",
    "\n",
    "experiment_config={'ExperimentName': experiment_name,\n",
    "                   'TrialName': trial_name,\n",
    "                   'TrialComponentDisplayName': 'Training'}\n",
    "\n",
    "# Train model\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation},\n",
    "        experiment_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deploy the Model\n",
    "\n",
    "After we have trained a model, we can deploy the model to an Amazon SageMaker endpoint to give us the ability to make predictions in real-time. To deploy the endpoint, we will use the [Model's](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) `deploy()` method. The deploy method uses several parameters. We provide the deploy method with an `instance_count` which specifies the number of compute instances to launch initially and `instance_type` which specifies the compute instance type. We then provide a `CSVSerilizer` which is used to serialize incoming data for our endpoint. Then we need an `endpoint_name` which is the name of our endpoint and must be unique to your account. Once the endpoint has been deployed (this usually takes 5 minutes or so) a [Predictor](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html) is returned. \n",
    "\n",
    "For further information on available [instance types](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html), [types of endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html), and [serializers](https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html) availalbe please click on the corresponding links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "endpoint_name = SOLUTION_PREFIX + \"-endpoint\"\n",
    "\n",
    "xgb_predictor = xgb.deploy( initial_instance_count = 1, \n",
    "                            instance_type = 'ml.m5.xlarge', \n",
    "                            serializer = CSVSerializer(),\n",
    "                            endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test the Model\n",
    "\n",
    "Our model has been deployed to a SageMaker endpoint, now we can make requests and receive predictions in real-time. In the model deployment step a [Predictor](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html) object was returned and saved in the xgb_predictor variable. We can use this predictor to make requests to the endpoint. To make predictions we will use the `predict()` method to pass in data from the test_data dataframe. The predictor will return a percentage. If the percentage is greater than 0.5, the patient is less likely to survive Non-Small Cell Lung Cancer (NSCLC). If the percentage is less than 0.5, the patient is more likely to survive NSCLC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def predict(data, rows=500):\n",
    "    l_class = []\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = xgb_predictor.predict(array).decode('utf-8')\n",
    "        \n",
    "        l_prob = predictions.split(',')\n",
    "    \n",
    "        for prob in l_prob:\n",
    "            if (float(prob)>0.5):\n",
    "                l_class.append(1)\n",
    "            else:\n",
    "                l_class.append(0)\n",
    "    \n",
    "    return l_class\n",
    "\n",
    "y_predict = predict(test_data.to_numpy()[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_predict)\n",
    "f1 = f1_score(y_test, y_predict, average='weighted')\n",
    "prec = precision_score(y_test, y_predict, average='weighted')\n",
    "rec = recall_score(y_test, y_predict, average='weighted')\n",
    "\n",
    "print('Accuracy: ', acc)\n",
    "print('F1 score: ', f1)\n",
    "print('Precision: ', prec)\n",
    "print('Recall: ', rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as skm\n",
    "\n",
    "cm = skm.confusion_matrix(y_test, y_predict)\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.title('Results for Survival of NSCLC')\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the results, perdictions that are 0 are more likely to survive NSCLC and perdictions that are labeled as 1 are less likely to survive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Clean up\n",
    "\n",
    "After you are done using this notebook, delete the model and the endpoint to avoid any incurring charges. Also we clean up the SageMaker experiment from the registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_model()\n",
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "experiment.delete_all(action=\"--force\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
