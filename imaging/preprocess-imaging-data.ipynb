{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sagemaker-experiments sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "import os, sys\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "suffix=uuid.uuid1().hex[:8] # to be used in resource names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a container image from the Dockerfile\n",
    "\n",
    "Here we need to build a custom Docker container image to handle the CT images of DICOM format. We are going to use `sm-docker` utility described in [Using the Amazon SageMaker Studio Image Build CLI to build container images from your Studio notebooks](https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/). Note that you need to follow the prerequisite in the blog to add the IAM policies to you SageMaker execution role.\n",
    "\n",
    "Be sure to use the image and tag name defined in `!sm-docker build` command. We will be replacing the placeholders in the Stepfunctions state machine definition json file with your bucket and image uri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd src\n",
    "sed -i \"s|##REGION##|{region}|g\" Dockerfile\n",
    "cat Dockerfile\n",
    "sm-docker build . --repository medical-image-processing-smstudio:1.3\n",
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_image_uri=f'{account_id}.dkr.ecr.{region}.amazonaws.com/medical-image-processing-smstudio:1.3'\n",
    "print(ecr_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up an experiment in SageMaker to hold information of the processing jobs. \n",
    "\n",
    "Execute the next four cells to launch the training jobs if this is the first time running the demo. There will be 162 processing jobs submitted in a for loop. We implemented a function `wait_for_instance_quota()` to check for the current job count and limit the total jobs in this experiment to `job_limit`. If the job count is at the limit, the function waits number of seconds specified in `wait` argument and check the job count again. This is to account for account level SageMaker quota that may cause error in the for loop. The default service quota for *Number of instances across processing jobs* and *number of ml.r5.large instances* are 4 as documented in [Service Quota page](https://docs.aws.amazon.com/general/latest/gr/sagemaker.html#limits_sagemaker). If your account has a higher limit, you may change the `job_limit` to a higher number to allow more simultaneous training jobs (therefore faster). You can also [request a quota increase](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from botocore.exceptions import ClientError\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "dict_processor = {}\n",
    "\n",
    "experiment_name = f'nsclc-lung-cancer-survival-prediction-{suffix}'\n",
    "trial_name = 'multimodal-1'\n",
    "\n",
    "try:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, \n",
    "        description='Lung cancer survival prediction using multi-modal Non Small Cell Lung Cancer (NSCLC) Radiogenomic dataset')\n",
    "except ClientError as e:\n",
    "    experiment = Experiment.load(experiment_name)\n",
    "    print(f'{experiment_name} experiment already exists! Reusing the existing experiment.')\n",
    "    \n",
    "# Creating a new trial for the experiment\n",
    "exp_trial = Trial.create(experiment_name=experiment_name, \n",
    "                             trial_name=trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_processing_job(subject, input_data_s3, output_data_s3, feature_store_name, offline_store_s3uri):\n",
    "    exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "    jobname = f'nsclc-{subject}-{exp_datetime}'\n",
    "\n",
    "    experiment_config={'ExperimentName': experiment_name,\n",
    "                       'TrialName': trial_name,\n",
    "                       'TrialComponentDisplayName': f'ImageProcessing-{subject}'}\n",
    "\n",
    "    inputs = [ProcessingInput(input_name = 'DICOM',\n",
    "                              source=f'{input_data_s3}/{subject}',\n",
    "                              destination='/opt/ml/processing/input')]\n",
    "\n",
    "    outputs = [ProcessingOutput(output_name=i,\n",
    "                                source='/opt/ml/processing/output/%s' % i,\n",
    "                                destination=os.path.join(output_data_s3, i)) \n",
    "               for i in ['CT-Nifti', 'CT-SEG', 'PNG']]\n",
    "\n",
    "    arguments = ['--subject', subject, \n",
    "                 '--feature_store_name', feature_store_name, \n",
    "                 '--offline_store_s3uri', offline_store_s3uri]\n",
    "    \n",
    "    script_processor = ScriptProcessor(command=['python3'],\n",
    "                                       image_uri=ecr_image_uri,\n",
    "                                       role=role,\n",
    "                                       instance_count=1,\n",
    "                                       instance_type='ml.m5.large',\n",
    "                                       volume_size_in_gb=5,\n",
    "                                       sagemaker_session=sagemaker_session)\n",
    "\n",
    "    script_processor.run(code='./src/dcm2nifti_processing.py',\n",
    "                         inputs=inputs,\n",
    "                         outputs=outputs,\n",
    "                         arguments=arguments,\n",
    "                         job_name=jobname,\n",
    "                         experiment_config=experiment_config,\n",
    "                         wait=False,\n",
    "                         logs=False)\n",
    "\n",
    "    return script_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_instance_quota(dict_processor, job_limit = 4, wait = 30):\n",
    "    def query_jobs(dict_processor):\n",
    "        counter=0\n",
    "        for key, processor in dict_processor.items():\n",
    "            status = processor.jobs[-1].describe()['ProcessingJobStatus']\n",
    "            # print(status)\n",
    "            time.sleep(2)\n",
    "            if status == \"InProgress\":\n",
    "                counter+=1\n",
    "        return counter\n",
    "    \n",
    "    job_count = query_jobs(dict_processor)\n",
    "    if job_count < job_limit:\n",
    "        print(f'Current total running jobs {job_count} is below {job_limit}. Proceeding...')\n",
    "        return \n",
    "    \n",
    "    while job_count >= job_limit:\n",
    "        print(f'Current total running jobs {job_count} is reaching the limit {job_limit}. Waiting {wait} seconds...')\n",
    "        time.sleep(wait)\n",
    "        job_count = query_jobs(dict_processor)\n",
    "\n",
    "    print(f'Current total running jobs {job_count} is below {job_limit}. Proceeding...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be running this workflow for all the `RO1` subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list = ['R01-%03d'%i for i in range(1,163)]\n",
    "\n",
    "input_data_bucket='multimodal-image-data' # this is where you downloaded the DICOM files\n",
    "input_data_prefix='nsclc_radiogenomics'\n",
    "\n",
    "output_data_bucket=default_bucket\n",
    "output_data_prefix='nsclc_radiogenomics'\n",
    "\n",
    "input_dicom_dir = f's3://{input_data_bucket}/{input_data_prefix}'\n",
    "output_nifti_dir = f's3://{output_data_bucket}/{output_data_prefix}'\n",
    "    \n",
    "feature_store_name = 'imaging-feature-group-%s' % suffix # Please append suffix if you want to create a unique feature group repetitively\n",
    "offline_store_s3uri = '%s/multimodal-imaging-featurestore' % output_nifti_dir\n",
    "\n",
    "for subject in subject_list:\n",
    "    print(subject)\n",
    "    wait_for_instance_quota(dict_processor, job_limit=4, wait=30)\n",
    "    dict_processor[subject] = launch_processing_job(subject, input_dicom_dir, output_nifti_dir, feature_store_name, offline_store_s3uri)\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
